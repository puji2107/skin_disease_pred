# -*- coding: utf-8 -*-
"""ORIGINAL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/original-82b75b71-de1d-427f-813e-1955c1c4a0a3.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250313/auto/storage/goog4_request%26X-Goog-Date%3D20250313T070003Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5799a5e785576528c20b91d7a97823dc139d6585ff617ab9c9c283c987a73e447ffa58115e78eb4099e7b7fa3dd10151d01e98e2c0d1ebaaef320ca74878a5b10ed53751f4dec8fee1624cf1bfee999feb9ec7eff3ea93a2213b704ec284e1ef1079eb7d24e990634c79a6f4be6127673bbc1eef625a46b3583f32d8f03408a6484baa5e52db10f6922b1ad0b83528890f8db593b34698f1fcfd7f6904f689e5d93f35579703c5c0e0abaf1f9f3d99f1e8a3d6abf16c07c0798ab3632e50d6449a5ebe16f9ab5d020919ce3fc635293642c42006c45fbe064489bcdfcea976d7317523168c7fb59af7b0013ba3f2e87b345e03d1c1cb82a17e8a353f95af3250
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

poojitha2107_data_set_path = kagglehub.dataset_download('poojitha2107/data-set')

print('Data source import complete.')

import tensorflow as tf
from tensorflow.keras import layers, models, mixed_precision
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import pandas as pd
import numpy as np
import os
from sklearn.utils import resample

# Enable mixed precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

def create_model(input_shape=(224, 224, 3), num_classes=7):
    base_model = tf.keras.applications.ResNet50V2(
        input_shape=input_shape,
        include_top=False,
        weights='imagenet'
    )

    # Freeze base model
    base_model.trainable = False

    model = tf.keras.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.BatchNormalization(),
        layers.Dense(256, dtype='float32'),  # Mixed precision requires float32 for final dense layers
        layers.BatchNormalization(),
        layers.Activation('relu'),
        layers.Dropout(0.4),
        layers.Dense(128, dtype='float32'),
        layers.BatchNormalization(),
        layers.Activation('relu'),
        layers.Dropout(0.3),
        layers.Dense(num_classes, activation='softmax', dtype='float32')
    ])

    return model

def prepare_balanced_data(metadata_path, image_dirs, img_size=(224, 224)):
    df = pd.read_csv(metadata_path)

    def get_image_path(row):
        image_id = str(row['image_id'])
        for dir_path in image_dirs:
            full_path = os.path.join(dir_path, f"{image_id}.jpg")
            if os.path.exists(full_path):
                return full_path
        return None

    df['file_path'] = df.apply(get_image_path, axis=1)
    df = df.dropna(subset=['file_path'])

    # Balance the dataset
    df_list = []
    min_samples = min(df['dx'].value_counts())

    for class_name in df['dx'].unique():
        df_class = df[df['dx'] == class_name]
        if len(df_class) < 1000:  # For minority classes
            n_samples = min(1000, len(df_class) * 3)  # Oversample but cap at 1000
        else:  # For majority classes
            n_samples = min(len(df_class), 1000)  # Cap at 1000

        df_balanced = resample(df_class,
                             replace=True if len(df_class) < n_samples else False,
                             n_samples=n_samples,
                             random_state=42)
        df_list.append(df_balanced)

    balanced_df = pd.concat(df_list)

    print("\nBalanced class distribution:")
    print(balanced_df['dx'].value_counts())

    # Split into train/validation
    train_df = balanced_df.sample(frac=0.8, random_state=42)
    val_df = balanced_df.drop(train_df.index)

    train_datagen = ImageDataGenerator(
        preprocessing_function=tf.keras.applications.resnet_v2.preprocess_input,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        zoom_range=0.15,
        horizontal_flip=True,
        vertical_flip=True,
        fill_mode='nearest'
    )

    val_datagen = ImageDataGenerator(
        preprocessing_function=tf.keras.applications.resnet_v2.preprocess_input
    )

    train_generator = train_datagen.flow_from_dataframe(
        dataframe=train_df,
        x_col="file_path",
        y_col="dx",
        batch_size=32,
        seed=42,
        shuffle=True,
        class_mode="categorical",
        target_size=img_size
    )

    validation_generator = val_datagen.flow_from_dataframe(
        dataframe=val_df,
        x_col="file_path",
        y_col="dx",
        batch_size=32,
        seed=42,
        shuffle=False,
        class_mode="categorical",
        target_size=img_size
    )

    return train_generator, validation_generator

def train_model(metadata_path, image_dirs, model_save_path='skin_lesion_model.keras'):
    train_generator, validation_generator = prepare_balanced_data(metadata_path, image_dirs)

    num_classes = len(train_generator.class_indices)
    model = create_model(num_classes=num_classes)

    # Initial training
    initial_learning_rate = 0.001
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            model_save_path,
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_accuracy',
            factor=0.2,
            patience=2,
            min_lr=1e-6,
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=5,
            restore_best_weights=True,
            verbose=1
        )
    ]

    print("\nPhase 1: Initial training...")
    history1 = model.fit(
        train_generator,
        epochs=10,
        validation_data=validation_generator,
        callbacks=callbacks,
        verbose=1
    )

    print("\nPhase 2: Fine-tuning...")
    base_model = model.layers[0]
    base_model.trainable = True

    # Freeze earlier layers
    for layer in base_model.layers[:-30]:  # Train only the last 30 layers
        layer.trainable = False

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate/10),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    history2 = model.fit(
        train_generator,
        epochs=10,
        validation_data=validation_generator,
        callbacks=callbacks,
        verbose=1
    )

    # Combine histories
    history = {}
    for key in history1.history:
        history[key] = history1.history[key] + history2.history[key]

    return model, history

# Main execution
base_dir = "/kaggle/input/data-set"
metadata_path = os.path.join(base_dir, "HAM10000_metadata.csv")
image_dirs = [
    os.path.join(base_dir, "HAM10000_images_part_1"),
    os.path.join(base_dir, "HAM10000_images_part_2")
]
model_save_path = "/kaggle/working/skin_lesion_model.keras"

model, history = train_model(metadata_path, image_dirs, model_save_path)

# Plot results
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history['accuracy'])
plt.plot(history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'])

plt.subplot(1, 2, 2)
plt.plot(history['loss'])
plt.plot(history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'])

plt.tight_layout()
plt.show()

# Print final metrics
final_train_acc = history['accuracy'][-1]
final_val_acc = history['val_accuracy'][-1]
print(f"\nFinal Training Accuracy: {final_train_acc:.4f}")
print(f"Final Validation Accuracy: {final_val_acc:.4f}")

# Add this after training, before plotting
best_epoch = np.argmax(history['val_accuracy'])
print("\nFinal Metrics:")
print(f"Best Training Accuracy: {history['accuracy'][best_epoch]*100:.2f}%")
print(f"Best Validation Accuracy: {max(history['val_accuracy'])*100:.2f}%")
print(f"Final Training Accuracy: {history['accuracy'][-1]*100:.2f}%")
print(f"Final Validation Accuracy: {history['val_accuracy'][-1]*100:.2f}%")